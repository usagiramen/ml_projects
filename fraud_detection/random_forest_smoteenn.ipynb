{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fraud Detection with SMOTEENN\n",
    "\n",
    "Anomaly Detection is one of popular machine learning topics and use cases. One of its challenges is classifying fraud with tiny number of actual fraud transactions, meaning imbalance number of fraudulent/legit transactions.\n",
    "\n",
    "Training a model on such dataset may cause biases towards one class, because there's so many legit transactions, the model is likely to predict the majority class only. One way to overcome that is oversampling.\n",
    "\n",
    "\n",
    "### Reading and References\n",
    "- [Detecting Finance Frauds with SMOTEENN](#https://towardsdatascience.com/detecting-financial-fraud-using-machine-learning-three-ways-of-winning-the-war-against-imbalanced-a03f8815cce9)\n",
    "- [Comparison between different sampling methods](#https://imbalanced-learn.readthedocs.io/en/stable/auto_examples/combine/plot_comparison_combine.html)\n",
    "- [Dealing with Imbalance Data](#https://towardsdatascience.com/methods-for-dealing-with-imbalanced-data-5b761be45a18)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.combine import SMOTEENN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise, we're using credit card data from [Kaggle](https://www.kaggle.com/mlg-ulb/creditcardfraud). Fraud transactions are marked as `1` in `Class` column. As you can see, there's approximately 1.7% of fraud transactions in this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.998273\n",
       "1    0.001727\n",
       "Name: Class, dtype: float64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data = pd.read_csv(\"data/creditcard.csv\")\n",
    "raw_data[\"Class\"].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's split the data into training and test sets with standard 80:20 ratio. Why? Seems like everyone is doing it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training data: 227845 rows\n",
      "test data: 56962 rows\n"
     ]
    }
   ],
   "source": [
    "# extract features and labels\n",
    "features = np.array(raw_data.drop([\"Time\", \"Class\", \"Amount\"], axis=1))\n",
    "labels = np.array(raw_data[\"Class\"])\n",
    "\n",
    "train_x, test_x, train_y, test_y = train_test_split(features, labels, test_size=0.2)\n",
    "\n",
    "print(\"training data: {} rows\".format(train_x.shape[0]))\n",
    "print(\"test data: {} rows\".format(test_x.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm going to use only RandomForestClassifier. Maybe some other days I'll do algorithm selection with other models e.g. Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_forest(train_x, test_x, train_y):\n",
    "    \"\"\"Execute random forest classifier.\"\"\"\n",
    "    \n",
    "    forest = RandomForestClassifier(\n",
    "        n_estimators=100,\n",
    "        criterion=\"gini\",\n",
    "        max_depth=5,\n",
    "        min_samples_split=2,\n",
    "        min_samples_leaf=1\n",
    "    )\n",
    "    \n",
    "    forest.fit(train_x, train_y)\n",
    "    predictions = forest.predict(test_x)\n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use Confusion Matrix to map out the False Positive and False Negatives. As a fraud detection model, it's crucial to **reduce number of False Negatives** - we don't want a model that flags actual fraud transactions as legit.\n",
    "\n",
    "As a second opinion, We also measure the model performance using Precision/Recall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def report(y_true, y_predict):\n",
    "    \"\"\"Show model performance report.\"\"\"\n",
    "\n",
    "    cm = confusion_matrix(y_true, y_predict)\n",
    "    print(\"[CONFUSION MATRIX]\")\n",
    "    print(\"True Positive: {}\\tFalse Positive: {}\".format(cm[0][0], cm[0][1]))\n",
    "    print(\"False Negative: {}\\tTrue Negative: {}\".format(cm[1][0], cm[1][1]))\n",
    "\n",
    "    # recall/precision.\n",
    "    print(\"\\n[PRECISION/RECALL]\")\n",
    "    print(classification_report(y_true, y_predict))\n",
    "    \n",
    "    print(\"\\nEND REPORT\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's give this a go.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CONFUSION MATRIX]\n",
      "True Positive: 56866\tFalse Positive: 3\n",
      "False Negative: 25\tTrue Negative: 68\n",
      "\n",
      "[PRECISION/RECALL]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     56869\n",
      "           1       0.96      0.73      0.83        93\n",
      "\n",
      "    accuracy                           1.00     56962\n",
      "   macro avg       0.98      0.87      0.91     56962\n",
      "weighted avg       1.00      1.00      1.00     56962\n",
      "\n",
      "\n",
      "END REPORT\n",
      "Wall time: 53.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "predictions = random_forest(train_x, test_x, train_y)\n",
    "\n",
    "report(test_y, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There you go. The model performed really well on legit transactions (Class `0`), but it made many False Negatives on fraud transaction (Class `1`). *It predicted many fraud transactions as legit*, and that's a pretty bad model because we want to minimize Type II errors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Oversampling using SMOTEENN\n",
    "\n",
    "#### Power of \"Fake Data\".\n",
    "\n",
    "In order to combat class imbalance, we can apply oversampling methods on our fraud transactions (Class `1`) data. Oversampling creates more synthetic datapoints based on existing fraud transactions attributes, so we have equal amount of data in both classes. This provides our Random Forest model more data to train, thus lesser bias towards predicting only legit transactions.\n",
    "\n",
    "We're gonna use **SMOTEENN** as our oversampling technique.\n",
    "\n",
    "#### What is SMOTEENN\n",
    "\n",
    "It's an oversampling technique derived by combining two methods: create synthetic samples using **SMOTE**, and smoothen the data using **ENN**.\n",
    "\n",
    "SMOTE **(Synthetic Minority Oversampling TEchnique)** is done using few steps:\n",
    "- Get the [K-Nearest Neighbours](#https://medium.com/@chiragsehra42/k-nearest-neighbors-explained-easily-c26706aa5c7f) of all fraud transactions into datapoints **x**\n",
    "- Draw a line between **x**\n",
    "- Create synthetic samples along the lines between **x**\n",
    "\n",
    "This technique tends to introduce noisy data into the dataset, because it creates synthetic datapoints along the relationship (lines) between fraud transactions KNN datapoints, it's possible to create data that are borderline close to legit transactions. This may hinder our model performance, but that's where ENN comes in.\n",
    "\n",
    "ENN **(Edited Nearest Neighbours)** removes any datapoints whose class is different than the majority within its nearest neighbours. This undersampling method cleans noisy data (especially borderline datapoints between different classes) by removing them (meaning it removes both legit and fraud transactions data) so the datapoints between legit and fraud are easily distinguishable.\n",
    "\n",
    "Let's go ahead and apply SMOTEENN on our dataset. Assuming `x` is our features, and `y` is our labels in our original credit card dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature size before SMOTEENN:  284807\n",
      "Feature size after SMOTEENN:  568222\n",
      "Wall time: 9min 22s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# extract features and labels\n",
    "x = raw_data.drop([\"Time\", \"Class\", \"Amount\"], axis=1).copy()\n",
    "y = raw_data[\"Class\"].copy()\n",
    "\n",
    "smote = SMOTEENN(sampling_strategy=\"auto\", random_state=42)\n",
    "sx, sy = smote.fit_sample(x, y)\n",
    "\n",
    "print(\"Feature size before SMOTEENN: \", len(x))\n",
    "print(\"Feature size after SMOTEENN: \", len(sx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training data: 397755 rows\n",
      "test data: 170467 rows\n"
     ]
    }
   ],
   "source": [
    "s_train_features, s_test_features, s_train_labels, s_test_labels = train_test_split(sx, sy, test_size=0.3)\n",
    "\n",
    "print(\"training data: {} rows\".format(s_train_features.shape[0]))\n",
    "print(\"test data: {} rows\".format(s_test_features.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...now let's retrain the model and see our results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CONFUSION MATRIX]\n",
      "True Positive: 85100\tFalse Positive: 330\n",
      "False Negative: 9122\tTrue Negative: 75915\n",
      "\n",
      "[PRECISION/RECALL]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      1.00      0.95     85430\n",
      "           1       1.00      0.89      0.94     85037\n",
      "\n",
      "    accuracy                           0.94    170467\n",
      "   macro avg       0.95      0.94      0.94    170467\n",
      "weighted avg       0.95      0.94      0.94    170467\n",
      "\n",
      "\n",
      "END REPORT\n",
      "Wall time: 1min 40s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "smoteenn_predictions = random_forest(s_train_features, s_test_features, s_train_labels)\n",
    "\n",
    "report(s_test_labels, smoteenn_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### END"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
