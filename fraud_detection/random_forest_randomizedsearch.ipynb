{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fraud Detection\n",
    "This exercise is a follow from the previous (Fraud Detection with SMOTEENN).\n",
    "\n",
    "Today, I wanna focus on **hyperparameters tuning**. This is an important process in tweaking the machine learning model to get the best possible parameters for maximum results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "from imblearn.combine import SMOTEENN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the sake of testing, I'm going to reuse the original dataset with no SMOTEENN over-sampling. So I have a smaller dataset, and hopefully this reduces total processing time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data from csv.\n",
    "raw_data = pd.read_csv(\"data/creditcard.csv\")\n",
    "\n",
    "# extract features and labels\n",
    "features = np.array(raw_data.drop([\"Time\", \"Amount\", \"Class\"], axis=1))\n",
    "labels = np.array(raw_data[\"Class\"])\n",
    "\n",
    "X = features\n",
    "y = labels\n",
    "\n",
    "# hold off SMOTEENN over-sampling for the time being.\n",
    "\n",
    "# smoteenn = SMOTEENN(sampling_strategy=\"auto\", random_state=42)\n",
    "# X, y = smoteenn.fit_sample(features, labels)\n",
    "\n",
    "# print(\"Feature size after SMOTEENN: \", len(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply PCA\n",
    "\n",
    "Normally I wouldn't apply PCA on a dataset with 28 features, but perhaps it can reduce computation time and efforts, and every second counts. I've checked before and found out I can reduce the dimension to 20 and still retain ~87% of the principle component, so let's go ahead with that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sum of Explained Variance Ratio: 0.874525524402183\n"
     ]
    }
   ],
   "source": [
    "pca = PCA(n_components=18)\n",
    "\n",
    "reduced_X = pca.fit_transform(X)\n",
    "print(\"Sum of Explained Variance Ratio: {}\".format(sum(pca.explained_variance_ratio_)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RandomizedSearchCV\n",
    "\n",
    "Here's the list of hyperparameters I would like to tune. I could use either **GridSearchCV** or **RandomizedSearchCV**. GridSearch would take ages but it would get me the best hyperparameters. Since we don't have ages, and we want to optimise between performance and computational time, let's use RandomizedSearch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest = RandomForestClassifier(random_state=42)\n",
    "\n",
    "param_distribution = { \n",
    "    \"n_estimators\": [100, 200],\n",
    "    # \"max_features\": [\"auto\", \"sqrt\", \"log2\"],\n",
    "    \"min_samples_split\": [2, 3, 4],\n",
    "    \"min_samples_leaf\": [1, 2, 3],\n",
    "    \"max_depth\" : [4, 5, 6],\n",
    "    \"criterion\" :['gini', 'entropy']\n",
    "}\n",
    "\n",
    "# split dataset to training and test set with 80:20 ratio.\n",
    "train_x, test_x, train_y, test_y = train_test_split(features, labels, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And away we go..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 5h 6min\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=5, error_score='raise-deprecating',\n",
       "                   estimator=RandomForestClassifier(bootstrap=True,\n",
       "                                                    class_weight=None,\n",
       "                                                    criterion='gini',\n",
       "                                                    max_depth=None,\n",
       "                                                    max_features='auto',\n",
       "                                                    max_leaf_nodes=None,\n",
       "                                                    min_impurity_decrease=0.0,\n",
       "                                                    min_impurity_split=None,\n",
       "                                                    min_samples_leaf=1,\n",
       "                                                    min_samples_split=2,\n",
       "                                                    min_weight_fraction_leaf=0.0,\n",
       "                                                    n_estimators='warn',\n",
       "                                                    n_jobs=None,\n",
       "                                                    oob_score=False,\n",
       "                                                    random_state=42, verbose=0,\n",
       "                                                    warm_start=False),\n",
       "                   iid='warn', n_iter=50, n_jobs=None,\n",
       "                   param_distributions={'criterion': ['gini', 'entropy'],\n",
       "                                        'max_depth': [4, 5, 6],\n",
       "                                        'min_samples_leaf': [1, 2, 3],\n",
       "                                        'min_samples_split': [2, 3, 4],\n",
       "                                        'n_estimators': [100, 200]},\n",
       "                   pre_dispatch='2*n_jobs', random_state=None, refit=True,\n",
       "                   return_train_score=False, scoring=None, verbose=0)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "cv_forest = RandomizedSearchCV(estimator=forest, param_distributions=param_distribution, cv=5, n_iter=50)\n",
    "cv_forest.fit(train_x, train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...oh god. I finished 2 episodes of Chernobyl, and it's still running!!\n",
    "\n",
    "Here's the best suggested hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_estimators': 200,\n",
       " 'min_samples_split': 2,\n",
       " 'min_samples_leaf': 1,\n",
       " 'max_depth': 6,\n",
       " 'criterion': 'entropy'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_forest.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9994864930105993"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_forest.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a new RandomForestClassifier with the best hyperparameters, and retrain the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_forest = RandomForestClassifier(\n",
    "    n_estimators=200,\n",
    "    min_samples_split=2,\n",
    "    min_samples_leaf=1,\n",
    "    max_depth=6,\n",
    "    criterion=\"entropy\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 3min 5s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "best_forest.fit(train_x, train_y)\n",
    "\n",
    "predictions = best_forest.predict(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CONFUSION MATRIX]\n",
      "True Positive: 56858\tFalse Positive: 3\n",
      "False Negative: 27\tTrue Negative: 74\n",
      "\n",
      "[PRECISION/RECALL]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     56861\n",
      "           1       0.96      0.73      0.83       101\n",
      "\n",
      "    accuracy                           1.00     56962\n",
      "   macro avg       0.98      0.87      0.92     56962\n",
      "weighted avg       1.00      1.00      1.00     56962\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cm = confusion_matrix(test_y, predictions)\n",
    "print(\"[CONFUSION MATRIX]\")\n",
    "print(\"True Positive: {}\\tFalse Positive: {}\".format(cm[0][0], cm[0][1]))\n",
    "print(\"False Negative: {}\\tTrue Negative: {}\".format(cm[1][0], cm[1][1]))\n",
    "\n",
    "# recall/precision.\n",
    "print(\"\\n[PRECISION/RECALL]\")\n",
    "print(classification_report(test_y, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "well shit..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes\n",
    "\n",
    "I'm unsure if the hyperparameters tuning process was slow due to `cv` argument. When `cv=5`, it's applying **(Stratified)K-Fold CV** onto the dataset, which seems unnecessary since RandomForest essentially applies bootstraping (divide original dataset into several chunks and train multiple decision trees with these chunks of sampled data), so there isn't a risk of overfitting the data that K-Fold is trying to mitigate.\n",
    "\n",
    "...but given that we're using an imbalance class dataset, it's possible the Stratified K-Fold will ensure equal proportion of legit and fraud transactions across all sample datasets (so I won't end up a sample training set of Class `0` / legit transactions data only)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### END"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
