{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fraud Detection with SMOTEENN\n",
    "\n",
    "Anomaly Detection is one of popular machine learning topics and use cases. A unique challenge to this scenario is classifying fraud with tiny number of actual fraud transactions. Most datasets have an imbalance number of fraudulent/legit transactions, training a model without any sampling methods applied will yield dangerous results (the model is biased towards one class given its biased training data).\n",
    "\n",
    "In this example, we demonstrate fraud detection using random forest classifier with SMOTEENN to balance our datasets.\n",
    "\n",
    "\n",
    "### Table of Content\n",
    "- [Import data](#Import-Data)\n",
    "- [Classify using Random Forest](#Classify-using-Random-Forest)\n",
    "- [Oversampling using SMOTEENN](#Oversampling-using-SMOTEENN)\n",
    "- [Conclusion](#...and-we-achieve-100%-precision/recall.-wait-wha-)\n",
    "\n",
    "### Reading and References\n",
    "- [Detecting Finance Frauds with SMOTEENN](#https://towardsdatascience.com/detecting-financial-fraud-using-machine-learning-three-ways-of-winning-the-war-against-imbalanced-a03f8815cce9)\n",
    "- [Comparison between different sampling methods](#https://imbalanced-learn.readthedocs.io/en/stable/auto_examples/combine/plot_comparison_combine.html)\n",
    "- [Dealing with Imbalance Data](#https://towardsdatascience.com/methods-for-dealing-with-imbalanced-data-5b761be45a18)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.combine import SMOTEENN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Data\n",
    "\n",
    "We're using credit card data from [Kaggle](https://www.kaggle.com/mlg-ulb/creditcardfraud). Fraud transactions are marked as `1` in `Class` column. As you can see, there's approximately 1.7% of fraud transactions in this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.998273\n",
       "1    0.001727\n",
       "Name: Class, dtype: float64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data = pd.read_csv(\"data/creditcard.csv\")\n",
    "raw_data[\"Class\"].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we split the data into training and test sets with 70:30 ratio and feed the training data to our random forest classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training data: 199364 rows\n",
      "test data: 85443 rows\n"
     ]
    }
   ],
   "source": [
    "def feature_label_split(data, label_name):\n",
    "    \"\"\"Split dataset to features and labels.\"\"\"\n",
    "    \n",
    "    labels = np.array(data[label_name])\n",
    "    features = np.array(data.drop(label_name, axis=1))\n",
    "    \n",
    "    return features, labels\n",
    "\n",
    "features, labels = feature_label_split(raw_data, label_name=\"Class\")\n",
    "train_x, test_x, train_y, test_y = train_test_split(features, labels, test_size=0.3)\n",
    "\n",
    "print(\"training data: {} rows\".format(train_x.shape[0]))\n",
    "print(\"test data: {} rows\".format(test_x.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classify using Random Forest\n",
    "\n",
    "I've written a wrapper function to initialise a Random Forest classifier model that takes in training features `train_x`, training labels `train_y`, and test features `test_x`. The random forest will fit the model using training data: `train_x` and `train_y`, and then predict a transaction is legit or fraud given test data `test_x`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_forest(train_x, test_x, train_y):\n",
    "    \"\"\"Execute random forest classifier.\"\"\"\n",
    "    \n",
    "    forest = RandomForestClassifier(\n",
    "        n_estimators=100,\n",
    "        criterion=\"gini\",\n",
    "        max_depth=5,\n",
    "        min_samples_split=2,\n",
    "        min_samples_leaf=1\n",
    "    )\n",
    "    \n",
    "    forest.fit(train_x, train_y)\n",
    "    predictions = forest.predict(test_x)\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "def compute_roc_auc(index):\n",
    "    y_predict = clf.predict_proba(X.iloc[index])[:,1]\n",
    "    fpr, tpr, thresholds = roc_curve(y.iloc[index], y_predict)\n",
    "    auc_score = auc(fpr, tpr)\n",
    "    return fpr, tpr, auc_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will then use the predictions from `random_forest` function to report the model's performance by matching its predictions to the actual labels `test_y`.\n",
    "\n",
    "We'll use Confusion Matrix to map out the False Positive and False Negatives. As a fraud detection model, it's crucial to **reduce number of False Negatives** - we don't want a model that flags actual fraud transactions as legit. We also measure the model performance using Precision/Recall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def report(y_true, y_predict):\n",
    "    \"\"\"Show model performance report.\"\"\"\n",
    "\n",
    "    cm = confusion_matrix(y_true, y_predict)\n",
    "    print(\"[CONFUSION MATRIX]\")\n",
    "    print(\"True Positive: {}\\tFalse Positive: {}\".format(cm[0][0], cm[0][1]))\n",
    "    print(\"False Negative: {}\\tTrue Negative: {}\".format(cm[1][0], cm[1][1]))\n",
    "\n",
    "    # recall/precision.\n",
    "    print(\"\\n[PRECISION/RECALL]\")\n",
    "    print(classification_report(y_true, y_predict))\n",
    "    \n",
    "    print(\"\\nEND REPORT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CONFUSION MATRIX]\n",
      "True Positive: 85273\tFalse Positive: 14\n",
      "False Negative: 42\tTrue Negative: 114\n",
      "\n",
      "[PRECISION/RECALL]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     85287\n",
      "           1       0.89      0.73      0.80       156\n",
      "\n",
      "    accuracy                           1.00     85443\n",
      "   macro avg       0.95      0.87      0.90     85443\n",
      "weighted avg       1.00      1.00      1.00     85443\n",
      "\n",
      "\n",
      "END REPORT\n",
      "Wall time: 56.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "predictions = random_forest(train_x, test_x, train_y)\n",
    "\n",
    "report(test_y, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the result, our performed really well on legit transactions (Class `0`). It didn't do so well on predicting fraud transactions (Class `1`) due to high number of false negatives (Type II Error) in our confusion matrix. Same results is shown on our precision/recall report (80% recall).\n",
    "\n",
    "This isn't a good model because *it predicted many fraud transactions as legit*, and that's a pretty bad model since we want to minimize Type II errors in our case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Oversampling using SMOTEENN\n",
    "\n",
    "#### Power of \"Fake Data\".\n",
    "\n",
    "In order to combat class imbalance, we can apply oversampling methods on our fraud transactions (Class `1`) data. Oversampling creates more synthetic datapoints based on existing fraud transactions attributes, so we have equal amount of data in both classes. This provides our Random Forest model more data to train, thus lesser bias towards predicting only legit transactions.\n",
    "\n",
    "We're gonna use **SMOTEENN** as our oversampling technique.\n",
    "\n",
    "#### What is SMOTEENN\n",
    "\n",
    "It's an oversampling technique derived by combining two methods: create synthetic samples using **SMOTE**, and smoothen the data using **ENN**.\n",
    "\n",
    "SMOTE **(Synthetic Minority Oversampling TEchnique)** is done using few steps:\n",
    "- Get the [K-Nearest Neighbours](#https://medium.com/@chiragsehra42/k-nearest-neighbors-explained-easily-c26706aa5c7f) of all fraud transactions into datapoints **x**\n",
    "- Draw a line between **x**\n",
    "- Create synthetic samples along the lines between **x**\n",
    "\n",
    "This technique tends to introduce noisy data into the dataset, because it creates synthetic datapoints along the relationship (lines) between fraud transactions KNN datapoints, it's possible to create data that are borderline close to legit transactions. This may hinder our model performance, but that's where ENN comes in.\n",
    "\n",
    "ENN **(Edited Nearest Neighbours)** removes any datapoints whose class is different than the majority within its nearest neighbours. This undersampling method cleans noisy data (especially borderline datapoints between different classes) by removing them (meaning it removes both legit and fraud transactions data) so the datapoints between legit and fraud are easily distinguishable.\n",
    "\n",
    "Let's go ahead and apply SMOTEENN on our dataset. Assuming `x` is our features, and `y` is our labels in our original credit card dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature size before SMOTEENN:  284807\n",
      "Feature size after SMOTEENN:  541135\n",
      "Wall time: 15.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "x = raw_data.drop(\"Class\", axis=1).copy()\n",
    "y = raw_data[\"Class\"].copy()\n",
    "\n",
    "smote = SMOTEENN(sampling_strategy=\"auto\", random_state=42)\n",
    "sx, sy = smote.fit_sample(x, y)\n",
    "\n",
    "print(\"Feature size before SMOTEENN: \", len(x))\n",
    "print(\"Feature size after SMOTEENN: \", len(sx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training data: 378794 rows\n",
      "test data: 162341 rows\n"
     ]
    }
   ],
   "source": [
    "s_train_features, s_test_features, s_train_labels, s_test_labels = train_test_split(sx, sy, test_size=0.3)\n",
    "\n",
    "print(\"training data: {} rows\".format(s_train_features.shape[0]))\n",
    "print(\"test data: {} rows\".format(s_test_features.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...now let's retrain the model and see our results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CONFUSION MATRIX]\n",
      "True Positive: 79381\tFalse Positive: 385\n",
      "False Negative: 4474\tTrue Negative: 78101\n",
      "\n",
      "[PRECISION/RECALL]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      1.00      0.97     79766\n",
      "           1       1.00      0.95      0.97     82575\n",
      "\n",
      "    accuracy                           0.97    162341\n",
      "   macro avg       0.97      0.97      0.97    162341\n",
      "weighted avg       0.97      0.97      0.97    162341\n",
      "\n",
      "\n",
      "END REPORT\n",
      "Wall time: 1min 35s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "smoteenn_predictions = random_forest(s_train_features, s_test_features, s_train_labels)\n",
    "\n",
    "report(s_test_labels, smoteenn_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### END"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
