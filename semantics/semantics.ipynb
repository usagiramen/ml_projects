{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "from numpy.random import randn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset:\n",
    "    def __init__(self, filename):\n",
    "        self.vocab_size = 0\n",
    "        \n",
    "        # read data from csv and process text to bool.\n",
    "        self.raw_data = self._read_data_from_csv(filename)\n",
    "        self.data = self._text_to_boolean(self.raw_data)\n",
    "        \n",
    "        # assign index to words.\n",
    "        self.word_idx = self._assign_word_to_index(self.data)\n",
    "\n",
    "\n",
    "    def _read_data_from_csv(self, filename):\n",
    "        \"\"\"Read raw data from CSV.\"\"\"\n",
    "\n",
    "        with open(filename) as file:\n",
    "            reader = csv.reader(file)\n",
    "            data = {row[1]:row[2] for row in reader}\n",
    "\n",
    "        return data\n",
    "    \n",
    "    \n",
    "    def _text_to_boolean(self, data):\n",
    "        \"\"\"Read the second column and turn True/False from text to boolean.\"\"\"\n",
    "\n",
    "        processed_data = {}\n",
    "\n",
    "        for text, label in data.items():\n",
    "            if label == \"True\":\n",
    "                processed_data[text] = True\n",
    "            else:\n",
    "                processed_data[text] = False\n",
    "\n",
    "        return processed_data\n",
    "\n",
    "\n",
    "    def _assign_word_to_index(self, data):\n",
    "        \"\"\"Preprocess data by assigning index to words.\"\"\"\n",
    "        \n",
    "        text = list(data.keys())\n",
    "        \n",
    "        # get all unique words in this dataset.\n",
    "        words = []\n",
    "        \n",
    "        for t in text:\n",
    "            word = t.split(\" \")\n",
    "            words.extend(word)\n",
    "            \n",
    "        vocabulary = list(set(words))\n",
    "        \n",
    "        # assign indices to each word.\n",
    "        word_to_idx = {w:i for i, w in enumerate(vocabulary)}\n",
    "        idx_to_word = {i:w for i, w in enumerate(vocabulary)}\n",
    "        \n",
    "        output = {\n",
    "            \"word_to_idx\": word_to_idx,\n",
    "            \"idx_to_word\": idx_to_word,\n",
    "        }\n",
    "        \n",
    "        # update vocabulary size.\n",
    "        self.vocab_size = len(vocabulary)\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    \n",
    "    def one_hot_vector(self, words):\n",
    "        \"\"\"Create one-hot vectors from words.\"\"\"\n",
    "\n",
    "        inputs = []\n",
    "        for word in words.split(\" \"):\n",
    "            # set all word indices for this vector to 0.\n",
    "            vector = np.zeros((self.vocab_size, 1))\n",
    "\n",
    "            # set the current word index as 1.\n",
    "            vector[word_to_idx[word.lower()]] = 1\n",
    "            inputs.append(vector)\n",
    "\n",
    "        return inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RNN class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN:\n",
    "    \"\"\"Vanilla Recurrent Neural Network.\"\"\"\n",
    "    \n",
    "    def __init__(self, input_size, output_size, hidden_size=64):\n",
    "        \"\"\"Initialise weights and bias.\"\"\"\n",
    "        \n",
    "        # weights.\n",
    "        self.Whh = randn(hidden_size, hidden_size) / 1000\n",
    "        self.Wxh = randn(hidden_size, input_size) / 1000\n",
    "        self.Why = randn(output_size, hidden_size) / 1000\n",
    "        \n",
    "        # biases.\n",
    "        self.bh = np.zeros((hidden_size, 1))\n",
    "        self.by = np.zeros((output_size, 1))\n",
    "        \n",
    "        # previous inputs and hidden states.\n",
    "        self.last_inputs = []\n",
    "        self.last_hidden_state = {}\n",
    "        \n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        \"\"\"Perform forward pass of RNN with given inputs.\n",
    "        \n",
    "        It returns the final output and hidden state.\n",
    "        Let 'y' be the output, and 'h' be the hidden state.\n",
    "        \"\"\"\n",
    "        \n",
    "        h = np.zeros((self.Whh.shape[0], 1))\n",
    "        \n",
    "        self.last_inputs = inputs\n",
    "        self.last_hidden_state = {0: h}\n",
    "        \n",
    "        # perform each step of RNN.\n",
    "        for i, x in enumerate(inputs):\n",
    "            h = np.tanh(self.Wxh @ x + self.Whh @ h + self.bh)\n",
    "            \n",
    "            self.last_hidden_state[i + 1] = h\n",
    "            \n",
    "        # compute output.\n",
    "        y = self.Why @ h + self.by\n",
    "            \n",
    "        return y, h\n",
    "\n",
    "        \n",
    "    def backprop(self, d_y, learn_rate=2e-2):\n",
    "        \"\"\"Perform back propogation of RNN.\n",
    "        \n",
    "        d_y (dL / dy) has a shape (output_size, 1).\n",
    "        learn_rate is a float.\n",
    "        \"\"\"\n",
    "\n",
    "        n = len(self.last_inputs)\n",
    "        \n",
    "        # calculate dl/dWhy and dl/dby.\n",
    "        d_Why = d_y @ self.last_hidden_state[n].T\n",
    "        d_by = d_y\n",
    "        \n",
    "        # initialise dl/dWhh, dL/dWxh, and dL/dbh to zero.\n",
    "        d_Whh = np.zeros(self.Whh.shape)\n",
    "        d_Wxh = np.zeros(self.Wxh.shape)\n",
    "        d_bh = np.zeros(self.bh.shape)\n",
    "        \n",
    "        # Calculate dL/dh for the previous h.\n",
    "        d_h = self.Why.T @ d_y\n",
    "        \n",
    "        # back propagate through time.\n",
    "        for t in reversed(range(n)):\n",
    "            # intermediate value: dL/dh * (1 - h^2).\n",
    "            temp = ((1 - self.last_hidden_state[t+1] ** 2) * d_h)\n",
    "            \n",
    "            # dL/db = dL/dh * (1 - h^2).\n",
    "            d_bh += temp\n",
    "            \n",
    "            # dL/dWhh = dl/dh * (1 - h^2) * h_{t-1}.\n",
    "            d_Whh += temp @ self.last_hidden_state[t].T\n",
    "            \n",
    "            # dL/dWxh = dL/dh * (1 - h^2) * x\n",
    "            d_Wxh += temp @ self.last_inputs[t].T\n",
    "            \n",
    "            # Next dL/dh = dL/dh * (1 - h^2) * Whh\n",
    "            d_h = self.Whh @ temp\n",
    "            \n",
    "        # clip to prevent exploding gradients.\n",
    "        for d in [d_Wxh, d_Whh, d_Why, d_bh, d_by]:\n",
    "            np.clip(d, -1, 1, out=d)\n",
    "            \n",
    "        # update weights and biases using gradient descent.\n",
    "        self.Whh -= learn_rate * d_Whh\n",
    "        self.Wxh -= learn_rate * d_Wxh\n",
    "        self.Why -= learn_rate * d_Why\n",
    "        self.bh -= learn_rate * d_bh\n",
    "        self.by -= learn_rate * d_by\n",
    "        \n",
    "\n",
    "    def softmax(self, array):\n",
    "        \"\"\"Applies softmax to the input array.\"\"\"\n",
    "\n",
    "        softmax = np.exp(self.array) / sum(np.exp(self.array))\n",
    "\n",
    "        return softmax\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### create training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = Dataset(\"data/train_data.csv\")\n",
    "test_data = Dataset(\"data/test_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(data, backprop=True):\n",
    "    \"\"\"Returns RNN's loss and accuracy for the given data.\n",
    "    \n",
    "    Args:\n",
    "        data (dict): mapping text to True/False (good/bad).\n",
    "        backprop (bool): run with back propagation.\n",
    "    \"\"\"\n",
    "    \n",
    "    items = list(data.items())\n",
    "    random.shuffle(items)\n",
    "\n",
    "    loss = 0\n",
    "    num_correct = 0\n",
    "    \n",
    "    for text, label in items:\n",
    "        inputs = one_hot_vector(text, len(vocab))\n",
    "        target = int(label)\n",
    "\n",
    "        # forward pass.\n",
    "        output, _ = rnn.forward(inputs)\n",
    "        probability = rnn.softmax(output)\n",
    "        \n",
    "        # calculate loss / accuracy.\n",
    "        loss -= np.log(probability[target])\n",
    "        num_correct += int(np.argmax(probability) == target)\n",
    "        \n",
    "        if backprop:\n",
    "            # build dL/dy.\n",
    "            dL_dy = probability\n",
    "            dL_dy[target] -= 1\n",
    "\n",
    "            # back propagation.\n",
    "            rnn.backprop(dL_dy)\n",
    "        \n",
    "    return loss/len(data), num_correct/len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Epoch 100\n",
      "Train:\tLoss 0.689 | Accuracy: 0.534\n",
      "Test:\tLoss 0.697 | Accuracy: 0.500\n",
      "-- Epoch 200\n",
      "Train:\tLoss 0.660 | Accuracy: 0.655\n",
      "Test:\tLoss 0.740 | Accuracy: 0.650\n",
      "-- Epoch 300\n",
      "Train:\tLoss 0.586 | Accuracy: 0.638\n",
      "Test:\tLoss 0.670 | Accuracy: 0.600\n",
      "-- Epoch 400\n",
      "Train:\tLoss 0.377 | Accuracy: 0.879\n",
      "Test:\tLoss 0.754 | Accuracy: 0.550\n",
      "-- Epoch 500\n",
      "Train:\tLoss 0.336 | Accuracy: 0.828\n",
      "Test:\tLoss 0.552 | Accuracy: 0.650\n",
      "-- Epoch 600\n",
      "Train:\tLoss 0.578 | Accuracy: 0.672\n",
      "Test:\tLoss 0.712 | Accuracy: 0.600\n",
      "-- Epoch 700\n",
      "Train:\tLoss 0.338 | Accuracy: 0.897\n",
      "Test:\tLoss 0.304 | Accuracy: 0.950\n",
      "-- Epoch 800\n",
      "Train:\tLoss 0.034 | Accuracy: 1.000\n",
      "Test:\tLoss 0.113 | Accuracy: 0.950\n",
      "-- Epoch 900\n",
      "Train:\tLoss 0.008 | Accuracy: 1.000\n",
      "Test:\tLoss 0.079 | Accuracy: 0.950\n",
      "-- Epoch 1000\n",
      "Train:\tLoss 0.004 | Accuracy: 1.000\n",
      "Test:\tLoss 0.028 | Accuracy: 1.000\n"
     ]
    }
   ],
   "source": [
    "# initialise homemade RNN.\n",
    "rnn = RNN(train_data.vocab_size, 2)\n",
    "\n",
    "# training loop.\n",
    "for epoch in range(1000):\n",
    "    train_loss, train_accuracy = process_data(train_data)\n",
    "    \n",
    "    if epoch % 100 == 99:\n",
    "        print(\"-- Epoch {}\".format(epoch + 1))\n",
    "        print(\"Train:\\tLoss %.3f | Accuracy: %.3f\" % (train_loss, train_accuracy))\n",
    "\n",
    "        test_loss, test_accuracy = process_data(test_data, backprop=False)\n",
    "        print(\"Test:\\tLoss %.3f | Accuracy: %.3f\" %(test_loss, test_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
